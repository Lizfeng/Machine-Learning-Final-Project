---
title: "Tree Trial"
author: "Takahiro Minami"
date: "2/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(class) # for kNN
library(rsample) # for CV 
library(arm) # for a few base features
library(partykit)
library(reldist) # gigni coefficient
library(tidyverse)
```

# Basic staff to build Conditional Inference Tree

## The whole US sample 


```{r} 
##Variables: 
#HTOTVAL: household income
#H_TENURE
#A_SEX: 1 = Male, 2 = Female
#A_AGE
#PRCITSHP: 1 = Native, born in US; 2 = Native, born in PR or US outlying area / Native, born abroad of US parent(s) / Foreign born, US cit by naturalization; 3 = Foreign born, not a US citizen
#PRDTRACE: 1 = White only; 2 = Black only; 3 = American Indian, Alaskan Native only (AI); 4 = Others
#PEHSPNON: Are you Spanish, Hispanic, or Latino?  1 = Yes 2 = No
#PENATVTY: Place of Birth 1 = U.S., 2 = Outside of U.S. 
#PEFNTVTY: Father's Place of Birth 1 = U.S., 2 = Outside of U.S. 
#PEMNTVTY: Mother's Place of Birth 1 = U.S., 2 = Outside of U.S. 
#A_MJOCC: Major Occupation Code 0-11, Merge 6 and 11 to 0 
#A_MJIND: Major Industry Code 0-14, merge 14 to 0, merge 7 to 12, merge 1 and 2
#GESTFIPS
#state
#state_full
```


## We need factorize variables

```{r}
df1 <- read.csv("MLData_Clean.csv")

```

```{r}
# clean data
df1[sapply(df1, is.integer)] <- lapply(df1[sapply(df1, is.integer)], as.factor)
df1$HTOTVAL <- as.integer(df1$HTOTVAL)/1000
df1$A_AGE <- as.integer(df1$A_AGE)
df1 <- select(df1,-c(X,state,state_full,GESTFIPS,A_MJOCC,H_TENURE,A_MJIND,A_AGE))
head(df1)
```

```{r}
# split the df into training and test set
set.seed(1234)
train <- sample(c(1:nrow(df1)),nrow(df1)*0.75,replace = FALSE) 
test <- c(1:nrow(df1))[!(c(1:nrow(df1)) %in% train)]
df_train <- df1[train,]
df_test <- df1[test,]
```





# Cross validation for deciding alpha
```{r}
# Develop function for CV (get MSE)

holdout_results <- function(splits,alpha) {
  # Fit the model to the training set
  mod <- ctree(HTOTVAL ~ .,
                data=analysis(splits),
                control = ctree_control(alpha=alpha,
                                        testtype="Bonferroni"))
  # Get MSE based on test set
  test <- assessment(splits)
  mse <- 
    mean((test$HTOTVAL - predict(mod,newdata = test))^2)
    
  mse
}

# set potential alpha
set.seed(1234)
a <-  seq(0.01, 0.1, 0.005)
cv_result <- data.frame(alpha=a)

# repeat CV for each alpha
for (i in 1:length(a)) {

 ## 10 folded CV
cv10 <- vfold_cv(data = df_train,v = 10) %>%
  mutate(results = pmap(list(splits,a[i]),holdout_results)) %>%
  unnest(results)

cv_result$mes[i] <- mean(cv10$results)
}

# optimal alpha
opt_alpha <- cv_result$alpha[cv_result$mes==min(cv_result$mes)]
opt_alpha
```

```{r}
# Baseline case
citree <- ctree(HTOTVAL ~ .,
                data=df_train,
                control = ctree_control(alpha=opt_alpha,
                                        testtype="Bonferroni"))
```

```{r, fig.width=30, fig.height=20}
# Plot
plot(citree)
```

# MES and Opportunity based Gini coefficient
```{r}
# mse
mes <- mean((df_test$HTOTVAL - predict(citree,newdata = df_test))^2)

# opportunity based gini coefficient
gini_opp <- gini(predict(citree,newdata = df_test))
```


# Basic staff to build Conditional Inference Random Forest

```{r}
ciforest <- 
  cforest(HTOTVAL ~ .,
          data=df_train,
	        control = ctree_control(alpha=0.05,
                                  testtype="Bonferroni"))
```

```{r}
# mse
predict_forest <- predict(ciforest,newdata = df_test,type = "response")

mes_forest <- mean((df_test$HTOTVAL - predict_forest)^2)

# opportunity based gini coefficient
gini_opp_forest <- gini(predict_forest)
```


